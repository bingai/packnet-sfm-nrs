########################################################################################################################
### ARCH
arch:
    seed: 42                   # Random seed for Pytorch/Numpy initialization
    min_epochs: 1              # Minimum number of epochs
    max_epochs: 5              # Maximum number of epochs

########################################################################################################################
### CHECKPOINT
checkpoint:
    filepath: './trained_models'         # Checkpoint filepath to save data
    save_top_k: -1           # Number of best models to save; set to -1 to save all the models of each epoch
    monitor: 'loss'         # Metric to monitor for logging
    monitor_index: 0        # Dataset index for the metric to monitor
    mode: 'auto'            # Automatically determine direction of improvement (increase or decrease)
    # s3_path: ''             # s3 path for AWS model syncing
    # s3_frequency: 1         # How often to s3 sync

########################################################################################################################
### SAVE
save:
    folder: '/save_output'      # Folder where data will be saved
    depth:
        rgb: True               # Flag for saving rgb images
        viz: True               # Flag for saving inverse depth map visualization
        npz: True               # Flag for saving numpy depth maps
        png: True               # Flag for saving png depth maps

########################################################################################################################
### MODEL
model:
    name: 'SelfSupModel'            # Training model
    checkpoint_path: './pretrained_models/PackNet01_MR_selfsup_D.ckpt'        # Checkpoint path for model saving
    
    ### MODEL.PARAMS
    params:
        crop: 'garg'                # Which crop should be used during evaluation
        min_depth: 0.0              # Minimum depth value to evaluate
        max_depth: 80.0             # Maximum depth value to evaluate
    
    ### MODEL.OPTIMIZER
    optimizer:
        name: 'Adam'
        depth:
            lr: 0.0002              # Depth learning rate
            weight_decay: 0.0       # Dept weight decay
        pose:
            lr: 0.0002              # Pose learning rate
            weight_decay: 0.0       # Pose weight decay
    
    ### MODEL.SCHEDULER
    scheduler:
        name: 'StepLR'
        step_size: 30               # Scheduler step size
        gamma: 0.5                  # Scheduler gamma value
        T_max: 20                   # Scheduler maximum number of iterations
    
    ### MODEL.LOSS#
    loss:
        num_scales: 4                   # Number of inverse depth scales to use
        progressive_scaling: 0.0        # Training percentage to decay number of scales
        flip_lr_prob: 0.5               # Probablity of horizontal flippping
        rotation_mode: 'euler'          # Rotation mode
        upsample_depth_maps: True       # Resize depth maps to highest resolution
        ssim_loss_weight: 0.85          # SSIM loss weight
        occ_reg_weight: 0.1             # Occlusion regularizer loss weight
        smooth_loss_weight: 0.001       # Smoothness loss weight
        C1: 1e-4                        # SSIM parameter
        C2: 9e-4                        # SSIM parameter
        photometric_reduce_op: 'min'    # Method for photometric loss reducing
        disp_norm: True                 # Inverse depth normalization
        clip_loss: 0.0                  # Clip loss threshold variance
        padding_mode: 'zeros'           # Photometric loss padding mode
        automask_loss: True             # Automasking to remove static pixels
        velocity_loss_weight: 0.1       # Velocity supervision loss weight

        supervised_method: 'sparse-l1'  # Method for depth supervision
        supervised_num_scales: 4        # Number of scales for supervised learning
        supervised_loss_weight: 0.9     # Supervised loss weight
    
    ### MODEL.DEPTH_NET
    depth_net:
        name: 'PackNet01'           # Depth network name
        version: '1A'
        checkpoint_path: ''         # Depth checkpoint filepath
        dropout: 0.0                # Depth network dropout
    
    ### MODEL.POSE_NET
    pose_net:
        name: 'PoseNet'             # Pose network name
        version: ''
        checkpoint_path: ''         # Pose checkpoint filepath
        dropout: 0.0                # Pose network dropout

########################################################################################################################
### DATASETS   
datasets:
    ### DATASETS.AUGMENTATION
    augmentation:
        image_shape: (64, 64)             # Image shape
        jittering: (0.2, 0.2, 0.2, 0.05)     # Color jittering values
    
    ### DATASETS.TRAIN
    train:
        batch_size: 4                   # Training batch size
        num_workers: 16                 # Training number of workers
        back_context: 1                 # Training backward context
        forward_context: 1              # Training forward context
        dataset: ['ValetParking']       # Training dataset
        path: ['./data/datasets/ValetParking_exp']       # Training data path
        # path: ['./data/datasets/ValetParking']       # Training data path
        split: ['data_splits/vp_sampling.txt']          # Training split
        depth_type: ['']                # Training depth type
        cameras: [[]]                   # Training cameras (double list, one for each dataset)
        repeat: [2]                     # Number of times training dataset is repeated per epoch
        num_logs: 2                     # Number of training images to log
    
    ### DATASETS.VALIDATION    
    validation:
        batch_size: 1                   # Validation batch size
        num_workers: 8                  # Validation number of workers
        back_context: 0                 # Validation backward context
        forward_context: 0              # Validation forward context
        dataset: ['ValetParking']
        path: ['./data/datasets/ValetParking_exp']
        # path: ['./data/datasets/ValetParking']       # Training data path
        split: ['data_splits/vp_sampling_validate.txt']
        depth_type: ['']
        cameras: [[]]                   # Validation cameras (double list, one for each dataset)
        num_logs: 2                     # Number of validation images to log
    
    ### DATASETS.TEST
    test:
        batch_size: 1                   # Test batch size
        num_workers: 8                  # Test number of workers
        back_context: 0                 # Test backward context
        forward_context: 0              # Test forward context
        dataset: ['ValetParking']
        path: ['./data/datasets/ValetParking_exp']
        # path: ['./data/datasets/ValetParking']       # Training data path
        split: ['data_splits/vp_sampling_test.txt']
        cameras: [[]]                   # Test cameras (double list, one for each dataset)
        num_logs: 2                     # Number of test images to log

